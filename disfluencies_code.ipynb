{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MYHD_47PdXBE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements\n",
        "\n",
        "\n",
        "*   A folder with:\n",
        "     * a subfolder with the transcription files\n",
        "     * and a CSV file with the metadata\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3o53OrbMhep1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Type of information retrieved here\n",
        "\n",
        "\n",
        "*   Number of utterances, tonal units, and tokens in all speech (patients, doctors, and companions) and by file (patients, doctors, and companions).\n",
        "*   Only with patient speeches: Number of disfluencies per utterance, number of disfluencies per tonal unit, number of disfluencies per patient.\n",
        "*   Number of utterances, tonal units, and tokens in patient speeches and per patient.\n",
        "*   Number and location of disfluency by disfluency type for each patient and for all patients.\n",
        "*   Number of disfluencies by disfluency type for each patient and for all patients.\n",
        "*   Plot showing the number of disfluencies (excluding interrupted utterances) by position.\n",
        "*   Function that generates plots to visualize the number of tokens and/or number of disfluencies based on a variable (sex, education, etc.).\n",
        "*   Plot of disfluencies by negative, positive, and general categories.\n",
        "*   Plot of the number of disfluencies by the number of tokens.\n",
        "*   A table where each row represents a patient: Unique patient ID, age, sex, education, medications, psychopathologies, number of tokens, total number of disfluencies, number of interrupted words, number of retractions, number of interrupted utterances, number of disfluencies at the beginning, number of disfluencies in the middle, number of disfluencies at the end, number of words before interruption.\n"
      ],
      "metadata": {
        "id": "o3bk7DWshx-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "WiFOUoNGidZK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcABuVUUpaWs"
      },
      "outputs": [],
      "source": [
        "# connect google drive to this collab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "m1__-VtF0G2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a txt file with the corpus information\n",
        "with open(\"corpus_information.txt\", \"w\") as corpus_info_txt:\n",
        "  corpus_info_txt.write(\"C-ORAL-ESQ SAMPLE INFORMATION\\n============\\n\\n\")"
      ],
      "metadata": {
        "id": "WgUzr0kwBdQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading metadata\n",
        "metadata = pd.read_csv('/content/drive/MyDrive/consultorias/c-oral-esq/metadata_coralesq_new.csv')"
      ],
      "metadata": {
        "id": "Wz9jqArcDCQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcriptionPath = '/content/drive/MyDrive/consultorias/c-oral-esq/transcripts'\n",
        "# the names of the txt files MUST be the same as the document names in the metadata table file"
      ],
      "metadata": {
        "id": "-P2scQoN0Wsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting a list of the paths of each file\n",
        "simpleTranscriptList = glob.glob(f'{transcriptionPath}/*.txt')"
      ],
      "metadata": {
        "id": "qLkrXBqu0UbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking list\n",
        "len(simpleTranscriptList)"
      ],
      "metadata": {
        "id": "tZ_ENDZ60ivV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanTranscription(text):\n",
        "  cleanText = re.sub('\\<|\\>', \"\", text)  # Remove angle brackets (<, >) from the text\n",
        "  # cleanText = re.sub('\\n', \"\", cleanText)  # Optional: Remove newline characters (\\n) from the text (currently disabled)\n",
        "  cleanText = re.sub('^ ', \"\", cleanText)  # Remove leading spaces from the text\n",
        "  cleanText = re.sub('\\s+$', \"\", cleanText)  # Remove trailing spaces from the text\n",
        "  cleanText = re.sub('\\s+', \" \", cleanText)  # Replace consecutive whitespace characters with a single space\n",
        "  return cleanText"
      ],
      "metadata": {
        "id": "SAhsiedi3jtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createListUtterance(text):\n",
        "  delimiter_pattern = r\"( //|\\+)\"  # Regular expression pattern to split the text based on delimiters (// or +)\n",
        "  listOfUnits = re.split(delimiter_pattern, text)  # Split the text into a list of units using the delimiter pattern\n",
        "  formattedListUtt = [\"\".join(pair) for pair in zip(listOfUnits[0::2], listOfUnits[1::2])]  # Combine pairs of units into formatted utterances\n",
        "  return formattedListUtt"
      ],
      "metadata": {
        "id": "SDCFO9rHBQS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_preprocessFile(TranscriptionList): #list\n",
        "    dfList = []  # List to store DataFrames\n",
        "    newTurnList = []  # List to store processed turn data\n",
        "    for transcriptionFilePath in TranscriptionList:\n",
        "        fileOpen = open(transcriptionFilePath, \"r\", encoding=\"utf-8\") # Open the file for reading\n",
        "        file_name = os.path.basename(transcriptionFilePath)  # Extract the file name from the path\n",
        "        file_name = re.sub(\"\\.txt\", \"\", file_name)  # Remove the file extension from the file name\n",
        "        text = fileOpen.readlines()  # Read all lines of the file into a list\n",
        "        for turn in text:\n",
        "            abb = turn.split(\":\")[0][1:]  # Extract the acronym from the turn by splitting at \":\" and removing the leading character (typically \"*\")\n",
        "            abb = re.sub(\"\\*\", \"\", abb)  # Remove any remaining asterisks from the acronym\n",
        "            turnClean = re.sub(\"\\*[A-Z]{3}: \", \"\", turn)  # Remove the \"*ABC: \" pattern from the turn using regex\n",
        "            turnClean = cleanTranscription(turnClean)  # Clean the turn using the cleanTranscription function\n",
        "            turnClean = re.sub(\" +\", \" \", turnClean)  # Replace multiple consecutive spaces with a single space in the cleaned turn\n",
        "            newTurnList.append([abb, file_name, turnClean])  # Add the processed turn data (acronym, file name, cleaned turn) to the newTurnList\n",
        "    textDf = pd.DataFrame(newTurnList, columns=[\"acronym\", \"file\", \"turn\"])  # Create a DataFrame from the newTurnList with column names\n",
        "    dfList.append(textDf)  # Append the DataFrame to the dfList\n",
        "    allSimpleUttDf = pd.concat(dfList)  # Concatenate all DataFrames in dfList into a single DataFrame\n",
        "    return allSimpleUttDf  # Return the final DataFrame\n"
      ],
      "metadata": {
        "id": "OG37v6UySjAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df = read_preprocessFile(simpleTranscriptList) #applying the function"
      ],
      "metadata": {
        "id": "whcd0fUvTFEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df[\"utterances\"] = all_df.turn.apply(lambda x: createListUtterance(x)) #splitting the turns as a list of tonal utterances"
      ],
      "metadata": {
        "id": "amoZ-6qpVqLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df = all_df.explode(\"utterances\") # one row = one utterance"
      ],
      "metadata": {
        "id": "NNhyDnY-Vvtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df[\"tonal_units\"] = all_df.utterances.str.split((\" /(?: |\\n)\")) #splitting the turns as a list of tonal units"
      ],
      "metadata": {
        "id": "-F_X7zQBTP2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df = all_df.explode(\"tonal_units\") # one row = one tonal unit"
      ],
      "metadata": {
        "id": "VPmQQF5mT4V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df[:10] #checking"
      ],
      "metadata": {
        "id": "373r3dwVBqs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df = all_df[all_df[\"tonal_units\"]!=\"\"]  # Filter out rows where \"tonal_units\" column is not an empty string\n",
        "all_df = all_df[all_df[\"tonal_units\"]!=\"\\n\"]  # Filter out rows where \"tonal_units\" column is not a newline character\n",
        "all_df = all_df[all_df[\"tonal_units\"]!=\" \"]  # Filter out rows where \"tonal_units\" column is not a single space\n",
        "\n",
        "all_df[\"tonal_units\"] = all_df[\"tonal_units\"].str.replace(\"\\n\", \"\")  # Remove newline characters from \"tonal_units\" column\n",
        "all_df[\"tonal_units\"] = all_df[\"tonal_units\"].str.replace(\"//\", \"\")  # Remove \"//\" characters from \"tonal_units\" column\n",
        "all_df[\"tonal_units\"] = all_df[\"tonal_units\"].astype(str)  # Convert \"tonal_units\" column to string data type\n",
        "\n",
        "all_df[\"tonal_units_clean\"] = all_df[\"tonal_units\"].str.replace(\"\\<|\\>\", \"\")\n",
        "all_df[\"tonal_units_clean\"] = all_df[\"tonal_units_clean\"].str.replace(\"\\[\\/[0-9]\\]\", \"\")\n",
        "all_df[\"tonal_units_clean\"] = all_df[\"tonal_units_clean\"].str.replace(\"\\&he\", \"\")\n",
        "all_df[\"tonal_units_clean\"] = all_df[\"tonal_units_clean\"].str.replace(\"\\+\", \"\")\n",
        "\n",
        "\n",
        "all_df[\"utterances_clean\"] = all_df[\"utterances\"].str.replace(\"\\&he\", \"\")\n",
        "all_df[\"utterances_clean\"] = all_df[\"utterances_clean\"].str.replace(\"\\<|\\>\", \"\")\n",
        "all_df[\"utterances_clean\"] = all_df[\"utterances_clean\"].str.replace(\"\\[\\/[0-9]\\]\", \"\")\n",
        "all_df[\"utterances_clean\"] = all_df[\"utterances_clean\"].str.replace(\"\\/\\/\", \"\")\n",
        "all_df[\"utterances_clean\"] = all_df[\"utterances_clean\"].str.replace(\"\\/\", \"\")\n",
        "all_df[\"utterances_clean\"] = all_df[\"utterances_clean\"].str.replace(\"\\s+\", \" \")\n",
        "all_df[\"utterances_clean\"] = all_df[\"utterances_clean\"].str.replace(\"\\+\", \"\")"
      ],
      "metadata": {
        "id": "kzLySl1_US2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "UMBfXCUkWpYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df['number_tokens'] = all_df['tonal_units_clean'].apply(lambda x: len(nltk.word_tokenize(x)))\n",
        "# all_df[\"tokens\"] = all_df[\"tonal_units_clean\"].apply(lambda x: x.split())\n",
        "# all_df[\"number_tokens\"] = all_df[\"tokens\"].apply(lambda x: len(x)) #getting the number fo tokens per tonal unit"
      ],
      "metadata": {
        "id": "XtYbOlYbUjoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus1_info1 = f\"The corpus (patient+doctors+companions) has {all_df['utterances'].nunique()} utterances; {all_df['tonal_units'].nunique()} tonal units; and {all_df['number_tokens'].sum()} tokens in total\\n=========\\n\"\n",
        "print(corpus1_info1)"
      ],
      "metadata": {
        "id": "DCkAXKY8BKpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus1_info2 = f\"(patient+doctors+companions)\\nthe number of utterances per file is:\\n{all_df.groupby('file')['utterances'].nunique()}\\n=========\\n\"\n",
        "print(corpus1_info2)"
      ],
      "metadata": {
        "id": "-AMrYVhTBMag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus1_info3 = f\"(patient+doctors+companions)\\nthe number of tonal units per file is:\\n{all_df.groupby('file')['tonal_units'].count()}\\n=========\\n\"\n",
        "print(corpus1_info3)"
      ],
      "metadata": {
        "id": "YOezvWhH5eF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"corpus_information.txt\", \"a\") as corpus_info_txt:\n",
        "  corpus_info_txt.write(corpus1_info1)\n",
        "  corpus_info_txt.write(corpus1_info2)\n",
        "  corpus_info_txt.write(corpus1_info3)"
      ],
      "metadata": {
        "id": "Pz8PAmIFCm94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def substring_word_percentage(string, substring):\n",
        "    clean = re.sub(\"\\[\\/[0-9]+\\]\", \"\", string)  # Remove \"[/number]\" patterns from the string\n",
        "    clean = re.sub(\"\\/\", \"\", string)  # Remove \"/\" characters from the string\n",
        "    word_count = len(clean.split())  # Count the number of words in the cleaned string\n",
        "    substring_matches = re.finditer(substring, string)  # Find all matches of the substring in the string\n",
        "    positions = []  # List to store the positions of substring matches\n",
        "    if word_count > 1:  # Check if the string has more than one word\n",
        "        for match in substring_matches:\n",
        "            start_position = match.start()  # Start position of the substring match\n",
        "            end_position = match.end()  # End position of the substring match\n",
        "            substring_length = end_position - start_position  # Length of the substring match\n",
        "            position = start_position / len(string)  # Calculate the position of the substring match relative to the length of the string\n",
        "            position = round((position * 100), 2)  # Convert the position to a percentage rounded to 2 decimal places\n",
        "            positions.append(position)  # Add the position to the positions list\n",
        "        return positions  # Return the list of positions of substring matches\n",
        "    else:\n",
        "        return \"one_word\"  # Return \"one_word\" if the string has only one word"
      ],
      "metadata": {
        "id": "eAnpslHDXV64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_disfluency_position(posit):\n",
        "    if posit == \"one_word\":\n",
        "        position = \"one_word\"\n",
        "    elif posit < 33.3:\n",
        "        position = \"beginning\"\n",
        "    elif 33.3 < posit < 66.6:\n",
        "        position = \"middle\"\n",
        "    else: # larger than 66.6\n",
        "        position = \"end\"\n",
        "    return position"
      ],
      "metadata": {
        "id": "33L2WSD-XuBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_disfluency(string):\n",
        "    unit = str(string)\n",
        "    disfluencies = []\n",
        "\n",
        "    # Find interrupted words\n",
        "    matchesInterruptedWord = re.findall(\"&(?!he)\\w+\", unit)\n",
        "    if matchesInterruptedWord: # &(?!he)\\w+\n",
        "        positions = substring_word_percentage(unit, \"&(?!he)\\w+\") # Calculates the positions of the matches using the substring_word_percentage function.\n",
        "        for position_number, match in zip(positions, matchesInterruptedWord): # Iterates over the positions and matches using zip.\n",
        "            if not isinstance(position_number, str):\n",
        "              posicao = check_disfluency_position(position_number) # Determines the position category using the check_disfluency_position function.\n",
        "              types = \"interrupted_word\" # Sets the disfluency type as \"interrupted_word\".\n",
        "              disfluencies.append(f\"{types}, {position_number}, {posicao}\") # Appends the disfluency occurrence to the disfluencies list.\n",
        "            else:\n",
        "              types = \"interrupted_word\" # Sets the disfluency type as \"interrupted_word\".\n",
        "              disfluencies.append(f\"{types}, one_word, one_word\") # Appends the disfluency occurrence to the disfluencies list.\n",
        "\n",
        "\n",
        "    # # Find filled pauses\n",
        "    matchesFilledPause = re.findall(\"&he\", unit)\n",
        "    if matchesFilledPause:\n",
        "        positions = substring_word_percentage(unit, \"&he\")\n",
        "        for position_number, match in zip(positions, matchesFilledPause): # Iterates over the positions and matches using zip.\n",
        "            if not isinstance(position_number, str):\n",
        "              posicao = check_disfluency_position(position_number) # Determines the position category using the check_disfluency_position function.\n",
        "              types = \"filled_pause\" # Sets the disfluency type as \"interrupted_word\".\n",
        "              disfluencies.append(f\"{types}, {position_number}, {posicao}\") # Appends the disfluency occurrence to the disfluencies list.\n",
        "            else:\n",
        "              types = \"filled_pause\" # Sets the disfluency type as \"interrupted_word\".\n",
        "              disfluencies.append(f\"{types}, one_word, one_word\") # Appends the disfluency occurrence to the disfluencies list.\n",
        "\n",
        "    # # Find retractings\n",
        "    matchesRetractings = re.findall(\"\\[\\/[0-9]\\]\", unit)\n",
        "    if matchesRetractings:\n",
        "        positions = substring_word_percentage(unit, \"\\[\\/[0-9]\\]\")\n",
        "        for position_number, match in zip(positions, matchesRetractings): # Iterates over the positions and matches using zip.\n",
        "            if not isinstance(position_number, str):\n",
        "              posicao = check_disfluency_position(position_number) # Determines the position category using the check_disfluency_position function.\n",
        "              types = \"retracting\" # Sets the disfluency type as \"interrupted_word\".\n",
        "              disfluencies.append(f\"{types}, {position_number}, {posicao}\") # Appends the disfluency occurrence to the disfluencies list.\n",
        "            else:\n",
        "              types = \"retracting\" # Sets the disfluency type as \"interrupted_word\".\n",
        "              disfluencies.append(f\"{types}, one_word, one_word\") # Appends the disfluency occurrence to the disfluencies list.\n",
        "\n",
        "    # # Find interrupted utterances\n",
        "    matchesInterruptedUtt = re.findall(\"\\+\", unit)\n",
        "    if matchesInterruptedUtt:\n",
        "        positions = substring_word_percentage(unit, \"\\+\")\n",
        "        for position_number, match in zip(positions, matchesInterruptedUtt): # Iterates over the positions and matches using zip.\n",
        "            if not isinstance(position_number, str):\n",
        "              posicao = check_disfluency_position(position_number) # Determines the position category using the check_disfluency_position function.\n",
        "              types = \"interrupted_utterance\" # Sets the disfluency type as \"interrupted_word\".\n",
        "              disfluencies.append(f\"{types}, {position_number}, {posicao}\") # Appends the disfluency occurrence to the disfluencies list.\n",
        "            else:\n",
        "              types = \"interrupted_utterance\" # Sets the disfluency type as \"interrupted_word\".\n",
        "              disfluencies.append(f\"{types}, one_word, one_word\") # Appends the disfluency occurrence to the disfluencies list.\n",
        "\n",
        "    return disfluencies\n"
      ],
      "metadata": {
        "id": "3OMlsMOCW34l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df[\"disfluencies\"] = all_df.tonal_units.apply(lambda x: find_disfluency(x)) #applying the function\n",
        "all_df = all_df.explode(\"disfluencies\") # one row = one disfluency"
      ],
      "metadata": {
        "id": "lKR2k9XbBVRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the disfluency type from the 'disfluencies' column and assign it to the 'disfluency_type' column\n",
        "all_df['disfluency_type'] = all_df['disfluencies'].str.split(\",\", expand=True)[0]\n",
        "\n",
        "# Extract the disfluency position percentage from the 'disfluencies' column and assign it to the 'disfluency_position_percentage' column\n",
        "all_df['disfluency_position_percentage'] = all_df['disfluencies'].str.split(\",\", expand=True)[1]\n",
        "\n",
        "# Extract the disfluency position from the 'disfluencies' column and assign it to the 'disfluency_position' column\n",
        "all_df['disfluency_position'] = all_df['disfluencies'].str.split(\",\", expand=True)[2]\n"
      ],
      "metadata": {
        "id": "F2vmmC4TdDv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in the NaN values with \"no_disfluency\"\n",
        "all_df['disfluencies'] = all_df['disfluencies'].fillna('no_disfluency')\n",
        "all_df['disfluency_type'] = all_df['disfluency_type'].fillna('no_disfluency')\n",
        "all_df['disfluency_position_percentage'] = all_df['disfluency_position_percentage'].fillna('no_disfluency')\n",
        "all_df['disfluency_position'] = all_df['disfluency_position'].fillna('no_disfluency_p')"
      ],
      "metadata": {
        "id": "pUJqSumVdjm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set 'disfluency_position' to \"not_applicable\" for rows where 'disfluency_type' is 'interrupted_utterance'\n",
        "all_df.loc[all_df['disfluency_type'] == 'interrupted_utterance', 'disfluency_position'] = \"not_applicable\"\n",
        "\n",
        "# Set 'disfluency_position_percentage' to \"not_applicable\" for rows where 'disfluency_type' is 'interrupted_utterance'\n",
        "all_df.loc[all_df['disfluency_type'] == 'interrupted_utterance', 'disfluency_position_percentage'] = \"not_applicable\"\n"
      ],
      "metadata": {
        "id": "glz3vyDXircB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column 'words_before_interruption' to the DataFrame\n",
        "# The value of this column is determined based on the conditions specified in the lambda function\n",
        "# counting the number of words in interrupted utterances\n",
        "all_df['words_before_interruption_unit'] = all_df.apply(lambda row: len(row['tonal_units_clean'].split('+')[0].split()) if row['disfluency_type'] == 'interrupted_utterance' else 'not_applicable', axis=1)\n",
        "all_df['words_before_interruption_utt'] = all_df.apply(lambda row: len(row['utterances_clean'].split('+')[0].split()) if row['disfluency_type'] == 'interrupted_utterance' else 'not_applicable', axis=1)"
      ],
      "metadata": {
        "id": "blDczated2Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_df[all_df.words_before_interruption_unit!=\"not_applicable\"] # checking"
      ],
      "metadata": {
        "id": "lGTzmYNeHLyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merging dataframes"
      ],
      "metadata": {
        "id": "I6cxXWixo7Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = metadata.fillna(\"X\") #putting Xs in the NaN values\n",
        "# Set the second row as the new header\n",
        "metadata.columns = metadata.iloc[0]\n",
        "# metadata = metadata.iloc[1:]"
      ],
      "metadata": {
        "id": "GpRh1obOCx-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the column\n",
        "metadata = metadata.rename(columns={'Document': 'file'})\n",
        "metadata = metadata.rename(columns={'Acronymname': 'acronym'})"
      ],
      "metadata": {
        "id": "zATwJwQVFG_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_df.file = all_df.file.str.replace(\"MED002\", \"MED_002\")\n",
        "# all_df.file = all_df.file.str.replace(\"MED004_2\", \"MED_004\")\n",
        "# Filter rows in dataframe_b based on conditions\n",
        "# merging the metadata and the texts\n",
        "filtered_patient = all_df.merge(metadata[metadata['Nature'] == 'Patient'],\n",
        "                               on=['file', 'acronym'],\n",
        "                               how='inner')"
      ],
      "metadata": {
        "id": "3ok4NvkPELC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting info about the patients"
      ],
      "metadata": {
        "id": "KCY03fKRpAgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus2_info1 = f'''there is {round(len(filtered_patient)/filtered_patient.utterances.nunique(), 2)} disfluencies per utterance\n",
        "And {round(len(filtered_patient)/filtered_patient.tonal_units.nunique(), 2)} per tonal unit.\n",
        "And {round(len(filtered_patient)/filtered_patient.file.nunique(), 2)} per patient.\\n=========\\n'''\n",
        "print(corpus2_info1)"
      ],
      "metadata": {
        "id": "zBBEk6j0TjTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus2_info2 = f\"The corpus (only patients) has {filtered_patient['utterances'].nunique()} utterances; {filtered_patient['tonal_units'].nunique()} tonal units; and {filtered_patient['number_tokens'].sum()} tokens in total\\n=========\\n\"\n",
        "print(corpus2_info2)"
      ],
      "metadata": {
        "id": "kYFbjv3TDW-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus2_info3 = f\"(patients)\\nthe number of utterances per file/patient is:\\n{filtered_patient.groupby('file')['utterances'].nunique()}\\n=========\\n\"\n",
        "print(corpus2_info3)"
      ],
      "metadata": {
        "id": "PUTeHXVuGUqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus2_info4 = f\"(patients)\\nthe number of tonal units per file/patient is:\\n{filtered_patient.groupby('file')['tonal_units'].count()}\\n=========\\n\"\n",
        "print(corpus2_info4)\n"
      ],
      "metadata": {
        "id": "LRTqkcyhGdWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus2_info5 = f\"(patients)\\nthe number of tokens per file/patient is:\\n{filtered_patient.groupby('file')['number_tokens'].count()}\\n=========\\n\"\n",
        "print(corpus2_info5)"
      ],
      "metadata": {
        "id": "ddzQsloLsH_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option(\"display.max_rows\", None)\n",
        "corpus2_info6 = f\"distribution of disfluency position per patient\\n{filtered_patient.groupby('disfluency_position')['file'].value_counts()}\\n======\\n\"\n",
        "print(corpus2_info6)"
      ],
      "metadata": {
        "id": "CBaSk9KWGoVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus2_info7 = f\"distribution of disfluency type per patient\\n{filtered_patient.groupby('disfluency_type')['file'].value_counts()}\\n======\\n\"\n",
        "print(corpus2_info7)"
      ],
      "metadata": {
        "id": "Q30yMW_RHb04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus2_info8 = f\"distribution of disfluency type in total\\n{filtered_patient.disfluency_type.value_counts()}=\\n=======\\n\"\n",
        "print(corpus2_info8)"
      ],
      "metadata": {
        "id": "YaAJgMARH1ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus2_info9 = f\"distribution of disfluency position in total\\n{filtered_patient.disfluency_position.value_counts()}\\n======\\n\"\n",
        "print(corpus2_info9)"
      ],
      "metadata": {
        "id": "cZCjKD4RIYrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the categories to exclude\n",
        "excluded_categories = ['no_disfluency_p', 'not_applicable']\n",
        "\n",
        "# Filter the DataFrame to exclude the categories\n",
        "df_disfluencies = filtered_patient[~filtered_patient['disfluency_position'].isin(excluded_categories)]"
      ],
      "metadata": {
        "id": "VepJC2JuJA_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by 'file_name' and 'disfluency_position' and calculate the counts\n",
        "counts = df_disfluencies.groupby(['file', 'disfluency_position']).size()\n",
        "\n",
        "# Group the data by 'file_name' and calculate the total counts for each file\n",
        "total_counts = df_disfluencies.groupby('file').size()\n",
        "\n",
        "# Calculate the percentage for each 'disfluency_position' in relation to data points of each file\n",
        "percentage = round((counts / counts.groupby(level=0).sum()) * 100, 2)"
      ],
      "metadata": {
        "id": "agtMXRUTGkG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus2_info10 = f\"the percentage of disfluency position per patient\\n{percentage}\\n=========\\n\"\n",
        "print(corpus2_info10)"
      ],
      "metadata": {
        "id": "5M21Gv9mJVz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"corpus_information.txt\", \"a\") as corpus_info_txt:\n",
        "  corpus_info_txt.write(corpus2_info1)\n",
        "  corpus_info_txt.write(corpus2_info2)\n",
        "  corpus_info_txt.write(corpus2_info3)\n",
        "  corpus_info_txt.write(corpus2_info4)\n",
        "  corpus_info_txt.write(corpus2_info5)\n",
        "  corpus_info_txt.write(corpus2_info6)\n",
        "  corpus_info_txt.write(corpus2_info7)\n",
        "  corpus_info_txt.write(corpus2_info8)\n",
        "  corpus_info_txt.write(corpus2_info9)\n",
        "  corpus_info_txt.write(corpus2_info10)"
      ],
      "metadata": {
        "id": "QM_uEPdcFWa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter specific categories\n",
        "filtered_data = filtered_patient[filtered_patient.disfluency_position.str.contains(\"beginning|middle|end\")]\n",
        "\n",
        "# Plot the filtered value counts\n",
        "filtered_data['disfluency_position'].value_counts().plot(kind='bar')\n",
        "# # Add labels and title\n",
        "plt.xlabel('Disfluency Position')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Value Counts of Disfluency Position (not interrupted utterances)')\n",
        "\n",
        "# # # Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2MQYlz0PIq-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining personal info about the speakers so that we can retrieve repeated speakers\n",
        "filtered_patient['combined_person_info'] = filtered_patient[['acronym', 'Agegroup', 'Sex', 'Schooling']].apply(lambda x: '_'.join(x.astype(str).str.replace(\"\\s\", \"\")), axis=1)"
      ],
      "metadata": {
        "id": "nd0o0Bw8KMh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_patient[\"tonal_units\"] = filtered_patient[\"tonal_units\"] + \"_endUnit\""
      ],
      "metadata": {
        "id": "_DpCuVWWIxHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a dataframe (one row = one patient)"
      ],
      "metadata": {
        "id": "k8MAxO9SpLxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by 'combined_person_info' and aggregate the data\n",
        "metadataText = filtered_patient.groupby('combined_person_info')[[\"file\", 'tonal_units', 'number_tokens',\n",
        "       'disfluencies', 'disfluency_type', 'disfluency_position_percentage',\n",
        "       'disfluency_position', 'words_before_interruption_unit', 'words_before_interruption_utt','Nature',\n",
        "       'Agegroup', 'Sex', 'Schooling', 'Medicine', 'P1-Delusions',\n",
        "       'P2-Conceptual Disorganization', 'P3- Hallucinatory Behavior',\n",
        "       'P4-Excitement', 'P5-Grandiosity', 'P6- Suspisciousness',\n",
        "       'P7-Hostility', 'SCALE TOTALpositive', 'N1-Afet. Embotada', 'N2-Retraim. Emoc',\n",
        "       'N3-Contato pobre', 'N4-Retraim. Social', 'N5-Dific. Pens. Abs.',\n",
        "       'N6-Falta Espontan.', 'N7-Pens. Estereoip.', 'SCALE TOTALnegative',\n",
        "       'G1-Preocup. Somát', 'G2-Ansiedade', 'G3-Culpa', 'G4-Tensão',\n",
        "       'G5-Maneirismo', 'G6-Depressão', 'G7-Retardo motor', 'G8-Falta Cooper.',\n",
        "       'G9-Pens. Incomum', 'G10-Desorientação', 'G11-Défict Atençao',\n",
        "       'G12-Juízo/Crítica', 'G13-Dist. Volição', 'G14-Mau Contr. Imp',\n",
        "       'G15-Preocupação', 'G16-Esquiva Social', 'SCALE TOTALgeneral']].agg(lambda x: ' '.join(map(str, x))).reset_index()"
      ],
      "metadata": {
        "id": "i9fhsvkeOzfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "# counting the number of tonal units per speaker\n",
        "metadataText[\"total_units\"] = metadataText['tonal_units'].apply(lambda x: x.split(\"_endUnit\"))\n",
        "metadataText[\"total_units\"] = metadataText[\"total_units\"].apply(lambda x: len(x))\n",
        "\n",
        "# counting the number of words per speaker\n",
        "metadataText[\"total_tokens\"] = metadataText['number_tokens'].apply(lambda x: x.split(\" \"))\n",
        "metadataText[\"total_tokens\"] = metadataText[\"total_tokens\"].apply(lambda x: sum(int(i) for i in x))\n",
        "\n",
        "# counting the number of disfluencies\n",
        "metadataText[\"total_disfluencies\"] = metadataText['disfluency_type'].apply(lambda x: x.split(\" \"))\n",
        "metadataText[\"total_disfluencies\"] = metadataText[\"total_disfluencies\"].apply(lambda x: len(x))\n",
        "\n",
        "# counting the number of disfluencies\n",
        "metadataText[\"total_disfluency_types\"] = metadataText['disfluency_type'].apply(lambda x: x.split(\" \"))\n",
        "metadataText[\"total_disfluency_types\"] = metadataText[\"total_disfluency_types\"].apply(lambda x:  Counter(x))"
      ],
      "metadata": {
        "id": "Y3DL0lTbPk4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand the dictionaries into separate columns\n",
        "expanded_data = metadataText['total_disfluency_types'].apply(pd.Series)\n",
        "\n",
        "# Concatenate the expanded data with the original DataFrame\n",
        "metadataText = pd.concat([metadataText, expanded_data], axis=1)"
      ],
      "metadata": {
        "id": "VAxY7LsOTTnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# counting the number of disfluency positions\n",
        "metadataText[\"total_disfluency_position\"] = metadataText['disfluency_position'].apply(lambda x: x.split())\n",
        "metadataText[\"total_disfluency_position\"] = metadataText[\"total_disfluency_position\"].apply(lambda x: Counter(x))\n",
        "# Expand the dictionaries into separate columns\n",
        "expanded_data = metadataText['total_disfluency_position'].apply(pd.Series)\n",
        "\n",
        "# Concatenate the expanded data with the original DataFrame\n",
        "metadataText = pd.concat([metadataText, expanded_data], axis=1)"
      ],
      "metadata": {
        "id": "LsWlxUb2Tlg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# counting the number of words in interrupted utterances\n",
        "metadataText[\"total_words_before_interruption\"] = metadataText['words_before_interruption_unit'].apply(lambda x: x.split())\n",
        "metadataText[\"total_words_before_interruption\"] = metadataText['words_before_interruption_unit'].apply(lambda x: sum(int(i) for i in x.split() if i.isnumeric()) if isinstance(x, str) else None)\n"
      ],
      "metadata": {
        "id": "c8WwhGXWVtb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update specific columns in the 'metadataText' DataFrame\n",
        "# The values in these columns are modified using the lambda function applied through 'applymap'\n",
        "# selecting the first item of the list (they're all the same)\n",
        "metadataText[['Agegroup', 'Sex', 'Schooling', 'Medicine']] = metadataText[['Agegroup', 'Sex', 'Schooling', 'Medicine']].applymap(lambda x: x.split(\" \")[0])"
      ],
      "metadata": {
        "id": "SjW2ActAZs04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The values in these columns are modified using the lambda function applied through 'applymap'\n",
        "# selecting the first item of the list (they're all the same)\n",
        "metadataText[['Medicine', 'P1-Delusions',\n",
        "       'P2-Conceptual Disorganization', 'P3- Hallucinatory Behavior',\n",
        "       'P4-Excitement', 'P5-Grandiosity', 'P6- Suspisciousness',\n",
        "       'P7-Hostility', 'SCALE TOTALpositive']] = metadataText[['Medicine', 'P1-Delusions',\n",
        "       'P2-Conceptual Disorganization', 'P3- Hallucinatory Behavior',\n",
        "       'P4-Excitement', 'P5-Grandiosity', 'P6- Suspisciousness',\n",
        "       'P7-Hostility', 'SCALE TOTALpositive']].applymap(lambda x: x.split(\" \")[0])"
      ],
      "metadata": {
        "id": "UHtynoz1ShzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The values in these columns are modified using the lambda function applied through 'applymap'\n",
        "# selecting the first item of the list (they're all the same)\n",
        "metadataText[['file', 'N1-Afet. Embotada', 'N2-Retraim. Emoc', 'N3-Contato pobre']] = metadataText[['file', 'N1-Afet. Embotada', 'N2-Retraim. Emoc',\n",
        "                                                                                'N3-Contato pobre']].applymap(lambda x: x.split(\" \")[0])\n",
        "\n",
        "metadataText[['N4-Retraim. Social', 'N5-Dific. Pens. Abs.', 'N6-Falta Espontan.', 'N7-Pens. Estereoip.', 'SCALE TOTALnegative']] = metadataText[['N4-Retraim. Social', 'N5-Dific. Pens. Abs.', 'N6-Falta Espontan.', 'N7-Pens. Estereoip.', 'SCALE TOTALnegative']].applymap(lambda x: x.split(\" \")[0])\n",
        "\n",
        "metadataText[['G1-Preocup. Somát', 'G2-Ansiedade', 'G3-Culpa', 'G4-Tensão']]= metadataText[['G1-Preocup. Somát', 'G2-Ansiedade', 'G3-Culpa', 'G4-Tensão']].applymap(lambda x: x.split(\" \")[0])\n",
        "\n",
        "metadataText[['G5-Maneirismo', 'G6-Depressão', 'G7-Retardo motor', 'G8-Falta Cooper.',\n",
        "       'G9-Pens. Incomum', 'G10-Desorientação', 'G11-Défict Atençao']] = metadataText[['G5-Maneirismo', 'G6-Depressão', 'G7-Retardo motor', 'G8-Falta Cooper.',\n",
        "                                                                                 'G9-Pens. Incomum', 'G10-Desorientação', 'G11-Défict Atençao']].applymap(lambda x: x.split(\" \")[0])\n",
        "\n",
        "metadataText[['G12-Juízo/Crítica', 'G13-Dist. Volição', 'G14-Mau Contr. Imp',\n",
        "       'G15-Preocupação', 'G16-Esquiva Social', 'SCALE TOTALgeneral']] = metadataText[['G12-Juízo/Crítica', 'G13-Dist. Volição', 'G14-Mau Contr. Imp',\n",
        "                                                                          'G15-Preocupação', 'G16-Esquiva Social', 'SCALE TOTALgeneral']].applymap(lambda x: x.split(\" \")[0])"
      ],
      "metadata": {
        "id": "GpcSXxmvbqUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadataText.columns"
      ],
      "metadata": {
        "id": "0bBd1rFDGfH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selecting columns and creating a new dataframe\n",
        "patientdisfluencies = metadataText[['file', 'combined_person_info', 'Agegroup', 'Sex', 'Schooling', 'Medicine', 'P1-Delusions', 'total_units',\n",
        "       'P2-Conceptual Disorganization', 'P3- Hallucinatory Behavior',\n",
        "       'P4-Excitement', 'P5-Grandiosity', 'P6- Suspisciousness',\n",
        "       'P7-Hostility', 'SCALE TOTALpositive',\n",
        "       'N1-Afet. Embotada', 'N2-Retraim. Emoc', 'N3-Contato pobre',\n",
        "       'N4-Retraim. Social', 'N5-Dific. Pens. Abs.', 'N6-Falta Espontan.',\n",
        "       'N7-Pens. Estereoip.', 'SCALE TOTALnegative',\n",
        "       'G1-Preocup. Somát', 'G2-Ansiedade', 'G3-Culpa', 'G4-Tensão',\n",
        "       'G5-Maneirismo', 'G6-Depressão', 'G7-Retardo motor', 'G8-Falta Cooper.',\n",
        "       'G9-Pens. Incomum', 'G10-Desorientação', 'G11-Défict Atençao',\n",
        "       'G12-Juízo/Crítica', 'G13-Dist. Volição', 'G14-Mau Contr. Imp',\n",
        "       'G15-Preocupação', 'G16-Esquiva Social', 'SCALE TOTALgeneral', 'total_tokens', 'total_disfluencies',\n",
        "       'total_disfluency_types', 'no_disfluency', 'interrupted_word',\n",
        "       'retracting', 'interrupted_utterance', 'total_disfluency_position',\n",
        "       'no_disfluency', 'beginning', 'middle', 'not_applicable', 'end',\n",
        "       'total_words_before_interruption']]"
      ],
      "metadata": {
        "id": "Usb9eFZacuq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column 'ratio_disfluency_tokens_percentage' to the 'patientdisfluencies' DataFrame\n",
        "# The values in this column are calculated based on the ratio of 'total_disfluencies' to 'total_tokens' multiplied by 100\n",
        "# The result is rounded to 2 decimal places using the 'round' function\n",
        "patientdisfluencies[\"ratio_disfluency_tokens_percentage\"] = round(patientdisfluencies[\"total_disfluencies\"]/patientdisfluencies[\"total_tokens\"]*100, 2)\n"
      ],
      "metadata": {
        "id": "98PrLVC5bAY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deleting the no_disfluency columns\n",
        "patientdisfluencies = patientdisfluencies.drop(columns='no_disfluency')"
      ],
      "metadata": {
        "id": "CV5FzCyJkFB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patientdisfluencies.columns #checking"
      ],
      "metadata": {
        "id": "KoG1Sa3Cjuir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patientdisfluencies.groupby(\"file\")[\"total_disfluencies\"].value_counts()"
      ],
      "metadata": {
        "id": "M_CoDr_8TqhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the specified columns to numeric, replacing invalid values with NaN\n",
        "cols_to_convert = ['SCALE TOTALpositive', 'SCALE TOTALnegative', 'SCALE TOTALgeneral']\n",
        "\n",
        "# Replace non-finite values with 0\n",
        "patientdisfluencies[cols_to_convert] = patientdisfluencies[cols_to_convert].apply(pd.to_numeric, errors='coerce')\n",
        "patientdisfluencies[cols_to_convert] = patientdisfluencies[cols_to_convert].fillna(0)\n",
        "\n",
        "# Convert the columns to integers\n",
        "patientdisfluencies[cols_to_convert] = patientdisfluencies[cols_to_convert].astype(int)"
      ],
      "metadata": {
        "id": "zfjI41H4abt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting"
      ],
      "metadata": {
        "id": "RjqjPkRQpUkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def groupAndPlotSum(df, column_a, column_b, bar_colors = None, file_name = None):\n",
        "    \"\"\"\n",
        "    Groups a pandas DataFrame by the values in column_a, calculates the sum of the\n",
        "    integer column values from column_b for each group, and returns a bar chart showing\n",
        "    the sum of each group.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas DataFrame\n",
        "        The DataFrame to be grouped.\n",
        "\n",
        "    column_a : str\n",
        "        The name of the column to group by.\n",
        "\n",
        "    column_b : str\n",
        "        The name of the column to calculate the sum of.\n",
        "\n",
        "    Returns: a bar plot, axis x = column_a values; axis y = grouped value\n",
        "    --------\n",
        "    None\n",
        "    \"\"\"\n",
        "    columnB = \" \".join(column_b.split(\"_\")[1:])\n",
        "    grouped = round((df.groupby(column_a)[column_b].sum()/df[column_b].sum())*100, 2)\n",
        "    # grouped = (df.groupby(column_a)[column_b].sum()/df.groupby(column_a)[\"total_words\"].sum())*100\n",
        "    grouped.plot(kind='bar', color=bar_colors)\n",
        "    # Set the y-axis tick label format to fixed notation with 2 decimal places\n",
        "    plt.ticklabel_format(axis='y', style='plain', useOffset=False, useMathText=True)\n",
        "    plt.title(f\"Percentage of {columnB} per {column_a}\")\n",
        "    plt.xlabel(column_a)\n",
        "    plt.ylabel(f\"Percentage of {columnB}\")\n",
        "    plt.savefig(f\"{file_name}.png\", bbox_inches='tight') #save the chart!\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "EUiyFJRKk9zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groupAndPlotSum(patientdisfluencies, \"Sex\", \"total_disfluencies\", bar_colors = [\"red\", \"blue\"], file_name = \"disfluencies_sex\")"
      ],
      "metadata": {
        "id": "fK3-GOped0dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groupAndPlotSum(patientdisfluencies, \"Sex\", \"total_tokens\", bar_colors = [\"red\", \"blue\"], file_name = \"tokens_sex\")"
      ],
      "metadata": {
        "id": "7sCe0Jn0lkNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "# Create a scatter plot\n",
        "sns.scatterplot(x='SCALE TOTALnegative', y='total_disfluencies', data=patientdisfluencies,\n",
        "                color='red')\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Score on negative')\n",
        "plt.ylabel('number of disfluencies')\n",
        "plt.title('Scale total on negative')\n",
        "\n",
        "# export figure as a PNG file\n",
        "plt.savefig(f\"scoreNegative_numberDisfluencies.png\", bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eAEy2D0cc45O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot\n",
        "sns.scatterplot(x='SCALE TOTALpositive', y='total_disfluencies', data=patientdisfluencies,\n",
        "                color='darkgreen')\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Score on positive')\n",
        "plt.ylabel('number of disfluencies')\n",
        "plt.title('Scale total on positive')\n",
        "\n",
        "# export figure as a PNG file\n",
        "plt.savefig(f\"scorePositive_numberDisfluencies.png\", bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8rd2q7wmnVAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot\n",
        "sns.scatterplot(x='SCALE TOTALgeneral', y='total_disfluencies', data=patientdisfluencies,\n",
        "                color='purple')\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Score on general')\n",
        "plt.ylabel('number of disfluencies')\n",
        "plt.title('Scale total on general')\n",
        "\n",
        "# export figure as a PNG file\n",
        "plt.savefig(f\"scoreGeneral_numberDisfluencies.png\", bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GINHp2JxncXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot\n",
        "sns.scatterplot(x='total_disfluencies', y='total_tokens', data=patientdisfluencies,\n",
        "                color='darkblue')\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('number of disfluencies')\n",
        "plt.ylabel('number of tokens')\n",
        "plt.title('Distribution of disfluencies per tokens')\n",
        "\n",
        "# export figure as a PNG file\n",
        "plt.savefig(f\"numberTokens_numberDisfluencies.png\", bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5VqheFY7TMZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patientdisfluencies.Medicine.value_counts()"
      ],
      "metadata": {
        "id": "iw2S4Nu5Y7iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot\n",
        "sns.scatterplot(x='Medicine', y='total_units', size = \"total_disfluencies\", data=patientdisfluencies,\n",
        "                color='darkblue')\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Number of disfluencies')\n",
        "plt.ylabel('Number of tonal units')\n",
        "plt.title('Distribution of disfluencies per medicine and tonal units')\n",
        "# Set legend position\n",
        "plt.legend(title='Legend', bbox_to_anchor=(1, 0.5), loc='center left')\n",
        "\n",
        "# Rotate x-axis labels\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "# export figure as a PNG file\n",
        "plt.savefig(f\"medicines_numberUnits_numberDisfluencies.png\", bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X2Yi7T7iuHSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# subdataframes to plot the interruptions\n",
        "filtered_patient_toplot_interruption = filtered_patient[filtered_patient['disfluencies']!=\"no_disfluency\"]\n",
        "filtered_patient_toplot_interruption = filtered_patient_toplot_interruption[filtered_patient_toplot_interruption['words_before_interruption_utt']!=\"not_applicable\"]\n",
        "filtered_patient_toplot_interruption = filtered_patient_toplot_interruption[filtered_patient_toplot_interruption['words_before_interruption_utt']>0]\n",
        "filtered_patient_toplot_interruption2 = filtered_patient_toplot_interruption[filtered_patient_toplot_interruption['words_before_interruption_unit']!=\"not_applicable\"]\n",
        "filtered_patient_toplot_interruption2 = filtered_patient_toplot_interruption[filtered_patient_toplot_interruption['words_before_interruption_unit']>0]"
      ],
      "metadata": {
        "id": "q2OBUhsrlEuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a histogram\n",
        "sns.histplot(data=filtered_patient_toplot_interruption, x='words_before_interruption_utt')\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Number of words before interruption')\n",
        "# plt.ylabel('Number of tokens')\n",
        "plt.title('Words in interrupted utterances (in utterances)')\n",
        "\n",
        "# export figure as a PNG file\n",
        "plt.savefig(f\"words_interrupteUtt_utterances.png\", bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ei5JQqCllKzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a histogram\n",
        "sns.histplot(data=filtered_patient_toplot_interruption2, x='words_before_interruption_unit', color = \"orange\")\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Number of words before interruption')\n",
        "# plt.ylabel('Number of tokens')\n",
        "plt.title('Words in interrupted utterances (in the last tonal unit)')\n",
        "# Adjust the x-axis scale\n",
        "plt.xlim(0, 20)  # Specify the desired lower and upper limits\n",
        "# export figure as a PNG file\n",
        "plt.savefig(f\"words_interrupteUtt_tonalUnits.png\", bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9TDlXSbdlQ3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(data=filtered_patient_toplot_interruption2, y='words_before_interruption_unit')\n",
        "\n",
        "\n",
        "# put a title\n",
        "plt.title('Words in interrupted utterances (in the last tonal unit)')\n",
        "\n",
        "# export figure as a PNG file\n",
        "plt.savefig(f\"words_interrupteUtt_tonalUnit_boxplot.png\", bbox_inches='tight')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sNL0DY7olWJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(data=filtered_patient_toplot_interruption, y='words_before_interruption_utt')\n",
        "\n",
        "\n",
        "# put a title\n",
        "plt.title('Words in interrupted utterances (in utterances)')\n",
        "\n",
        "# export figure as a PNG file\n",
        "plt.savefig(f\"words_interrupteUtt_utterances_boxplot.png\", bbox_inches='tight')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U5lGlDPHlajl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}